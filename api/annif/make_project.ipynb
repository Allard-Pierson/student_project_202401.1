{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AAI Project Bibliotheek\n",
    "## Annif Models\n",
    "### Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the training of multiple models within Annif with our LCSH datasets. For a detailed explanation for the creation of our dataset, please refer to [data_processing.ipynb](data_processing.py) Addtionally this notebook will also contain the list of actions needed to make a project in Annif, such as loading a vocabulary. The steps of making a project in Annif are also detailed by the exercises in the [Annif-tutorial repository](https://github.com/NatLibFi/Annif-tutorial/blob/master/exercises/02_tfidf_project.md). A lot of steps which are performed in this notebook are also detailed in the following [paper about implementing Annif with LCSH](https://publications.drdo.gov.in/ojs/index.php/djlit/article/view/18619/7915)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "#### Install tools\n",
    "First I will install the following tools: Annif & Skosify. Annif is for subject indexing and Skosify is used for cleaning the LCSH vocabulary file. You could also make a development install of Annif by cloning the repository and opening it in a Docker container. That way custom backends can be used in Annif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install annif\n",
    "%pip install skosify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add subject vocabulary to project\n",
    "I added the Library of Congress Subject Headings as my vocabulary, which can be found at the following [link](https://id.loc.gov/authorities/subjects.html). Here I downloaded the SKOS/RDF .ttl under bulk downloads\n",
    "Because this vocabulary can contain a lot of redundant information, duplicates and other inconsistencies, the tool [Skosify](https://github.com/NatLibFi/Skosify) will be used to clean this file. This takes a while.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads subjects.skosrdf.ttl, writes SKOS file lc_subjects.ttl and sets the concept scheme to \"Subjects\"\n",
    "!skosify subjects.skosrdf.ttl -o lc_subjects.ttl --label \"Subjects\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Load the new file in the Annif project as a vocabulary. This only has to be done once. And can be reused for the following projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "!annif load-vocab lcsh lc_subjects.ttl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the LCSH vocabulary is added to Annif."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make __projects.cfg__\n",
    "\n",
    "The next step is to make a project in Annif. To initialise a project, a __projects.cfg__ has to be made.\n",
    "The parameters can be different for each available backend / model, so do make sure to check te Annif-tutorial repository for the initialization of certain backends.\n",
    "\n",
    "Three backend will be tested and evaluated in this notebook: \n",
    "\n",
    "- [__TF-IDF__](https://monkeylearn.com/blog/what-is-tf-idf/): Short for _Term Frequency-Inverse Document Frequency_, it calculates two scores for each word: _term frequency_ (The frequency of a word in a document, indicating how often it occurs) and _Inverse document frequency_(Indicates how much a word occurs across all documents).\n",
    "- [__Omikuji-Parabel__](https://github.com/tomtung/omikuji): An implementation of a Partitioned Label Trees (Tree based machine learning algorithm) for extreme multi-label classification, such as the large amount of subjects in our assigment.\n",
    "- [__XTransformer__](https://github.com/amzn/pecos/blob/mainline/pecos/xmc/xtransformer/README.md): A module from the [PECOS](https://github.com/amzn/pecos/tree/mainline) framework using transformer models for extreme multi-label classification.\n",
    "\n",
    "`[lcsh-tfidf-en]`<br>\n",
    "`name=LCSH TFIDF Titles project` <br>\n",
    "`language=en`<br>\n",
    "`backend=tfidf`<br>\n",
    "`vocab=lcsh`<br>\n",
    "`analyzer=snowball(english)`\n",
    "\n",
    "`[lcsh-tfidf2-en]`<br>\n",
    "`name=LCSH TFIDF Summaries project` <br>\n",
    "`language=en`<br>\n",
    "`backend=tfidf`<br>\n",
    "`vocab=lcsh`<br>\n",
    "`analyzer=snowball(english)`\n",
    "\n",
    "`[lcsh-omikuji-parabel-en]`<br>\n",
    "`name=LCSH Omikuji Parabel project` <br>\n",
    "`language=en`<br>\n",
    "`backend=omikuji`<br>\n",
    "`vocab=lcsh`<br>\n",
    "`analyzer=snowball(english)`\n",
    "\n",
    "`[lcsh-xtransformer-distilbert-en]`<br>\n",
    "`name=LCSH XTransformer Distilbert project` <br>\n",
    "`language=en`<br>\n",
    "`backend=xtransformer`<br>\n",
    "`vocab=lcsh`<br>\n",
    "`analyzer=snowball(english)`\n",
    "\n",
    "Execute the following command to view the new projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID                       Project Name                          Vocabulary ID  Language  Trained  Modification time\n",
      "--------------------------------------------------------------------------------------------------------------------------\n",
      "lcsh-tfidf-en                    LCSH TFIDF Titles project             lcsh           en        False    -                \n",
      "lcsh-tfidf2-en                   LCSH TFIDF Summaries project          lcsh           en        False    -                \n",
      "lcsh-omikuji-parabel-en          LCSH Omikuji Parabel project          lcsh           en        False    -                \n",
      "lcsh-xtransformer-distilbert-en  LCSH XTransformer Distilbert project  lcsh           en        False    -                \n"
     ]
    }
   ],
   "source": [
    "# Command for listing projects\n",
    "!annif list-projects "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train & Evaluate Models\n",
    "Training will be done seperately for each model, Each model will also not be trained on the exact same size, since the full size of some datasets has proven te be too computationally intensive. The use of a specific datasets may also vary, since one contains only titles and other summaries as well.\n",
    "To evaluate the models, I have made a relatively small test dataset from the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be viewed in the code below, the test dataset is only 2% of the total dataset. My reasoning behind this is because I want as much data to train the models, to cover as much subjects as possible. The 2% of data still contains a lot of records, since the datasets are quite big."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reads original .tsv files\n",
    "summaries = pd.read_csv(\"datasets/summaries_uri.tsv\", delimiter='\\t', header=None)\n",
    "titles = pd.read_csv(\"datasets/titles_uri.tsv\", delimiter='\\t', header=None)\n",
    "\n",
    "# Splitting summaries into train set and test set.\n",
    "train_summaries, test_summaries = train_test_split(summaries, test_size=0.02)\n",
    "train_titles, test_titles = train_test_split(titles, test_size=0.02)\n",
    "\n",
    "# Exports new train & test datasets\n",
    "train_summaries.to_csv(\"datasets/train_summaries_uri.tsv\", sep='\\t', index=False, header=None)\n",
    "test_summaries.to_csv(\"datasets/test_summaries_uri.tsv\", sep='\\t', index=False, header=None)\n",
    "train_titles.to_csv(\"datasets/train_titles_uri.tsv\", sep='\\t', index=False, header=None)\n",
    "test_titles.to_csv(\"datasets/test_titles_uri.tsv\", sep='\\t', index=False, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During evaluation a plethora of metrics will be produced, including [precision, recall](https://en.wikipedia.org/wiki/Precision_and_recall) and [F1-scores](https://en.wikipedia.org/wiki/F-score). \n",
    "\n",
    "One particular metric which will also be analysed is [NDCG](https://towardsdatascience.com/demystifying-ndcg-bee3be58cfe0), which stands for _Normalized Discounted Cumulative Gain_. This metric is used to measure ranking quality (which are like the subjects that are produced), and is therefore often used to evaluate the performance of search engines or recommendation systems. It works by assigning a score to the relevance of each subject towards the query, these scores are then discounted based on the position in the search result. Because this is a good measurement for our task, we will be primarily looking at this metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TF-IDF\n",
    "The TF-IDF will be trained twice seperately, once on the dataset with titles only, and another time with the summaries.\n",
    "\n",
    "First with the titles dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backend tfidf: transforming subject corpus\n",
      "Backend tfidf: creating vectorizer\n",
      "\u001b[33mwarning: \u001b[0mUnknown subject URI <http://id.loc.gov/authorities/subjects/sh2023001686>\n",
      "\u001b[33mwarning: \u001b[0mUnknown subject URI <http://id.loc.gov/authorities/subjects/sh2023002506>\n",
      "\u001b[33mwarning: \u001b[0mUnknown subject URI <http://id.loc.gov/authorities/subjects/sh2023002505>\n",
      "\u001b[33mwarning: \u001b[0mUnknown subject URI <http://id.loc.gov/authorities/subjects/sh2023001641>\n",
      "Backend tfidf: creating similarity index\n"
     ]
    }
   ],
   "source": [
    "!annif train lcsh-tfidf-en datasets/train_titles_uri.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mwarning: \u001b[0mUnknown subject URI <http://id.loc.gov/authorities/subjects/sh2023001686>\n",
      "Precision (doc avg):          \t0.0419\n",
      "Recall (doc avg):             \t0.3037\n",
      "F1 score (doc avg):           \t0.0722\n",
      "Precision (subj avg):         \t0.0011\n",
      "Recall (subj avg):            \t0.0020\n",
      "F1 score (subj avg):          \t0.0012\n",
      "Precision (weighted subj avg):\t0.2308\n",
      "Recall (weighted subj avg):   \t0.2883\n",
      "F1 score (weighted subj avg): \t0.2177\n",
      "Precision (microavg):         \t0.0416\n",
      "Recall (microavg):            \t0.2883\n",
      "F1 score (microavg):          \t0.0728\n",
      "F1@5:                         \t0.1005\n",
      "NDCG:                         \t0.2148\n",
      "NDCG@5:                       \t0.1930\n",
      "NDCG@10:                      \t0.2148\n",
      "Precision@1:                  \t0.1430\n",
      "Precision@3:                  \t0.0878\n",
      "Precision@5:                  \t0.0652\n",
      "True positives:               \t2692\n",
      "False positives:              \t61946\n",
      "False negatives:              \t6647\n",
      "Documents evaluated:          \t6560\n"
     ]
    }
   ],
   "source": [
    "!annif eval lcsh-tfidf-en datasets/test_titles_uri.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the metrics, the TF-IDF model trained purely on title data is not performing that great. It has a NDCG score of 0.21 which is quite low. There is room for improvement.\n",
    "\n",
    "Next a separate TF-IDF model will be trained on the summaries dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backend tfidf: transforming subject corpus\n",
      "Backend tfidf: creating vectorizer\n",
      "Backend tfidf: creating similarity index\n"
     ]
    }
   ],
   "source": [
    "!annif train lcsh-tfidf2-en datasets/train_summaries_uri.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision (doc avg):          \t0.1442\n",
      "Recall (doc avg):             \t0.4487\n",
      "F1 score (doc avg):           \t0.2072\n",
      "Precision (subj avg):         \t0.0006\n",
      "Recall (subj avg):            \t0.0014\n",
      "F1 score (subj avg):          \t0.0007\n",
      "Precision (weighted subj avg):\t0.2759\n",
      "Recall (weighted subj avg):   \t0.4199\n",
      "F1 score (weighted subj avg): \t0.3006\n",
      "Precision (microavg):         \t0.1442\n",
      "Recall (microavg):            \t0.4199\n",
      "F1 score (microavg):          \t0.2147\n",
      "F1@5:                         \t0.2272\n",
      "NDCG:                         \t0.3574\n",
      "NDCG@5:                       \t0.3030\n",
      "NDCG@10:                      \t0.3574\n",
      "Precision@1:                  \t0.2994\n",
      "Precision@3:                  \t0.2361\n",
      "Precision@5:                  \t0.1971\n",
      "True positives:               \t6328\n",
      "False positives:              \t37562\n",
      "False negatives:              \t8741\n",
      "Documents evaluated:          \t4389\n"
     ]
    }
   ],
   "source": [
    "!annif eval lcsh-tfidf2-en datasets/test_summaries_uri.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model already produces a better NDCG score of 0.35. Although this is better, there is still room for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Omikuji Parabel\n",
    "The next model is Omikuji Parabel, as described before this is a tree-based model which is well suited for extreme multi-label classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backend omikuji: creating vectorizer\n",
      "Backend omikuji: creating train file\n",
      "2024-01-14T12:01:58.955Z \u001b[36mINFO \u001b[0m [omikuji::data] Loading data from data/projects/lcsh-omikuji-parabel-en/omikuji-train.txt\n",
      "2024-01-14T12:01:59.911Z \u001b[36mINFO \u001b[0m [omikuji::data] Parsing data\n",
      "2024-01-14T12:02:01.076Z \u001b[36mINFO \u001b[0m [omikuji::data] Loaded 215048 examples; it took 2.12s\n",
      "2024-01-14T12:02:01.202Z \u001b[36mINFO \u001b[0m [omikuji::model::train] Training model with hyper-parameters HyperParam { n_trees: 3, min_branch_size: 100, max_depth: 20, centroid_threshold: 0.0, collapse_every_n_layers: 0, linear: HyperParam { loss_type: Hinge, eps: 0.1, c: 1.0, weight_threshold: 0.1, max_iter: 20 }, cluster: HyperParam { k: 2, balanced: true, eps: 0.0001, min_size: 2 }, tree_structure_only: false, train_trees_1_by_1: false }\n",
      "2024-01-14T12:02:01.202Z \u001b[36mINFO \u001b[0m [omikuji::model::train] Initializing tree trainer\n",
      "2024-01-14T12:02:01.216Z \u001b[36mINFO \u001b[0m [omikuji::model::train] Computing label centroids\n",
      "Labels 14677 / 14677 [====================================] 100.00 % 21263.96/s --] 0.65 % 29511.48/s 7s ----------------] 1.03 % 23173.05/s 9s 86 / 215048 [>-------------------------------] 1.53 % 21927.38/s 10s -----------------------] 2.06 % 22367.12/s 9s ---------------------------] 2.34 % 21709.57/s 10s -] 2.66 % 18290.20/s 11s --------------------] 2.90 % 17419.53/s 12s ples 7787 / 215048 [=>------------------------------] 3.62 % 16389.38/s 13s 93 % 15781.14/s 13s -] 4.33 % 15618.02/s 13s --] 4.65 % 15617.40/s 13s --------------------] 5.15 % 15855.34/s 13s 215048 [=>-----------------------------] 6.06 % 16446.60/s 12s les 14070 / 215048 [==>----------------------------] 6.54 % 16495.45/s 12s -] 8.19 % 17280.26/s 11s  [===>---------------------------] 9.93 % 18182.36/s 11s -] 22.11 % 19366.52/s 9s --------------------] 22.46 % 19268.14/s 9s =====>--------------------] 35.13 % 19475.42/s 7s 50 % 19185.75/s 6s 25 % 19102.90/s 4s ===============>----------] 65.31 % 18987.32/s 4s 6619 / 215048 [============================>-] 96.08 % 19079.13/s 0s ====>---------------------] 37.40 % 23138.41/s 0s 2024-01-14T12:02:13.709Z \u001b[36mINFO \u001b[0m [omikuji::model::train] Start training forest\n",
      "45561 / 45561 [=============================================] 100.00 % 325.50/s 2024-01-14T12:04:33.683Z \u001b[36mINFO \u001b[0m [omikuji::model::train] Model training complete; it took 152.48s\n",
      "2024-01-14T12:04:33.762Z \u001b[36mINFO \u001b[0m [omikuji::model] Saving model...\n",
      "2024-01-14T12:04:33.788Z \u001b[36mINFO \u001b[0m [omikuji::model] Saving tree to data/projects/lcsh-omikuji-parabel-en/omikuji-model/tree0.cbor\n",
      "2024-01-14T12:04:35.914Z \u001b[36mINFO \u001b[0m [omikuji::model] Saving tree to data/projects/lcsh-omikuji-parabel-en/omikuji-model/tree1.cbor\n",
      "2024-01-14T12:04:38.018Z \u001b[36mINFO \u001b[0m [omikuji::model] Saving tree to data/projects/lcsh-omikuji-parabel-en/omikuji-model/tree2.cbor\n",
      "2024-01-14T12:04:40.039Z \u001b[36mINFO \u001b[0m [omikuji::model] Model saved; it took 6.28s\n"
     ]
    }
   ],
   "source": [
    "!annif train lcsh-omikuji-parabel-en datasets/train_summaries_uri.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-14T12:04:54.105Z \u001b[36mINFO \u001b[0m [omikuji::model] Loading model from data/projects/lcsh-omikuji-parabel-en/omikuji-model...\n",
      "2024-01-14T12:04:54.105Z \u001b[36mINFO \u001b[0m [omikuji::model] Loading model settings from data/projects/lcsh-omikuji-parabel-en/omikuji-model/settings.json...\n",
      "2024-01-14T12:04:54.111Z \u001b[36mINFO \u001b[0m [omikuji::model] Loaded model settings Settings { n_features: 336785, classifier_loss_type: Hinge }...\n",
      "2024-01-14T12:04:54.114Z \u001b[36mINFO \u001b[0m [omikuji::model] Loading tree from data/projects/lcsh-omikuji-parabel-en/omikuji-model/tree0.cbor...\n",
      "2024-01-14T12:04:57.135Z \u001b[36mINFO \u001b[0m [omikuji::model] Loading tree from data/projects/lcsh-omikuji-parabel-en/omikuji-model/tree1.cbor...\n",
      "2024-01-14T12:05:00.300Z \u001b[36mINFO \u001b[0m [omikuji::model] Loading tree from data/projects/lcsh-omikuji-parabel-en/omikuji-model/tree2.cbor...\n",
      "2024-01-14T12:05:03.305Z \u001b[36mINFO \u001b[0m [omikuji::model] Loaded model with 3 trees; it took 9.20s\n",
      "Precision (doc avg):          \t0.2186\n",
      "Recall (doc avg):             \t0.6580\n",
      "F1 score (doc avg):           \t0.3121\n",
      "Precision (subj avg):         \t0.0006\n",
      "Recall (subj avg):            \t0.0014\n",
      "F1 score (subj avg):          \t0.0008\n",
      "Precision (weighted subj avg):\t0.2460\n",
      "Recall (weighted subj avg):   \t0.6367\n",
      "F1 score (weighted subj avg): \t0.3469\n",
      "Precision (microavg):         \t0.2186\n",
      "Recall (microavg):            \t0.6367\n",
      "F1 score (microavg):          \t0.3255\n",
      "F1@5:                         \t0.3887\n",
      "NDCG:                         \t0.5867\n",
      "NDCG@5:                       \t0.5374\n",
      "NDCG@10:                      \t0.5867\n",
      "Precision@1:                  \t0.5924\n",
      "Precision@3:                  \t0.4356\n",
      "Precision@5:                  \t0.3408\n",
      "True positives:               \t9595\n",
      "False positives:              \t34295\n",
      "False negatives:              \t5474\n",
      "Documents evaluated:          \t4389\n"
     ]
    }
   ],
   "source": [
    "!annif eval lcsh-omikuji-parabel-en datasets/test_summaries_uri.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model has a greater performance than the TF-IDF models as expected since it is well suited for extreme multi-label classification. It has a NDCG score of 0.5867, which is a big improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XTransformer\n",
    "Another model which can be used is XTransformer. This is a custom backend mentioned in this [pull request](https://github.com/NatLibFi/Annif/pull/540). It adds the ability to use a transformer based model from huggingface and use it for extreme multi-label classification using the [PECOS](https://github.com/amzn/pecos/blob/mainline/pecos/xmc/xtransformer/README.md) framework. This backend has been modified by us so that it can be used in the current Annif version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads the training summaries data.\n",
    "data = pd.read_csv(\"datasets/train_summaries_uri.tsv\", delimiter='\\t', header=None)\n",
    "# Takes the first 50000 entries of the data.\n",
    "data_subset = data.head(50000)\n",
    "\n",
    "# Exports the dataframe to a .tsv file.\n",
    "data_subset.to_csv(\"datasets/xtf_train_summaries_uri.tsv\", sep='\\t', index=False, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This training might take longer than two hours..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backend xtransformer: creating vectorizer\n",
      "Backend xtransformer: creating training file\n",
      "Backend xtransformer: Start training\n",
      "Downloaded distilbert-base-multilingual-cased model from s3.\n",
      "INFO:pecos.xmc.xtransformer.matcher:Downloaded distilbert-base-multilingual-cased model from s3.\n",
      "***** Encoding data len=50000 truncation=128*****\n",
      "INFO:pecos.xmc.xtransformer.matcher:***** Encoding data len=50000 truncation=128*****\n",
      "***** Finished with time cost=7.673071384429932 *****\n",
      "INFO:pecos.xmc.xtransformer.matcher:***** Finished with time cost=7.673071384429932 *****\n",
      "trn tensors saved to /tmp/tmpj13lyfc5/X_trn.pt\n",
      "INFO:pecos.xmc.xtransformer.matcher:trn tensors saved to /tmp/tmpj13lyfc5/X_trn.pt\n",
      "Start fine-tuning transformer matcher...\n",
      "INFO:pecos.xmc.xtransformer.matcher:Start fine-tuning transformer matcher...\n",
      "/usr/local/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "INFO:pecos.xmc.xtransformer.matcher:***** Running training *****\n",
      "  Num examples = 50000\n",
      "INFO:pecos.xmc.xtransformer.matcher:  Num examples = 50000\n",
      "  Num labels = 2\n",
      "INFO:pecos.xmc.xtransformer.matcher:  Num labels = 2\n",
      "  Num Epochs = 3\n",
      "INFO:pecos.xmc.xtransformer.matcher:  Num Epochs = 3\n",
      "  Learning Rate Schedule = linear\n",
      "INFO:pecos.xmc.xtransformer.matcher:  Learning Rate Schedule = linear\n",
      "  Batch size = 32\n",
      "INFO:pecos.xmc.xtransformer.matcher:  Batch size = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "INFO:pecos.xmc.xtransformer.matcher:  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4689\n",
      "INFO:pecos.xmc.xtransformer.matcher:  Total optimization steps = 4689\n",
      "| [   1/   3][   100/  4689] |   99/1563 batches | ms/batch 402.2552 | train_loss 1.291302e-01 | lr 9.786735e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   1/   3][   100/  4689] |   99/1563 batches | ms/batch 402.2552 | train_loss 1.291302e-01 | lr 9.786735e-05\n",
      "| [   1/   3][   200/  4689] |  199/1563 batches | ms/batch 413.5304 | train_loss 1.136766e-01 | lr 9.573470e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   1/   3][   200/  4689] |  199/1563 batches | ms/batch 413.5304 | train_loss 1.136766e-01 | lr 9.573470e-05\n",
      "| [   1/   3][   300/  4689] |  299/1563 batches | ms/batch 396.8357 | train_loss 1.229248e-01 | lr 9.360205e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   1/   3][   300/  4689] |  299/1563 batches | ms/batch 396.8357 | train_loss 1.229248e-01 | lr 9.360205e-05\n",
      "| [   1/   3][   400/  4689] |  399/1563 batches | ms/batch 372.4957 | train_loss 1.144078e-01 | lr 9.146940e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   1/   3][   400/  4689] |  399/1563 batches | ms/batch 372.4957 | train_loss 1.144078e-01 | lr 9.146940e-05\n",
      "| [   1/   3][   500/  4689] |  499/1563 batches | ms/batch 375.5781 | train_loss 1.228285e-01 | lr 8.933675e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   1/   3][   500/  4689] |  499/1563 batches | ms/batch 375.5781 | train_loss 1.228285e-01 | lr 8.933675e-05\n",
      "| [   1/   3][   600/  4689] |  599/1563 batches | ms/batch 380.7575 | train_loss 1.185715e-01 | lr 8.720409e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   1/   3][   600/  4689] |  599/1563 batches | ms/batch 380.7575 | train_loss 1.185715e-01 | lr 8.720409e-05\n",
      "| [   1/   3][   700/  4689] |  699/1563 batches | ms/batch 373.7888 | train_loss 1.129838e-01 | lr 8.507144e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   1/   3][   700/  4689] |  699/1563 batches | ms/batch 373.7888 | train_loss 1.129838e-01 | lr 8.507144e-05\n",
      "| [   1/   3][   800/  4689] |  799/1563 batches | ms/batch 341.2788 | train_loss 1.282245e-01 | lr 8.293879e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   1/   3][   800/  4689] |  799/1563 batches | ms/batch 341.2788 | train_loss 1.282245e-01 | lr 8.293879e-05\n",
      "| [   1/   3][   900/  4689] |  899/1563 batches | ms/batch 354.9689 | train_loss 1.194181e-01 | lr 8.080614e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   1/   3][   900/  4689] |  899/1563 batches | ms/batch 354.9689 | train_loss 1.194181e-01 | lr 8.080614e-05\n",
      "| [   1/   3][  1000/  4689] |  999/1563 batches | ms/batch 348.0195 | train_loss 1.189408e-01 | lr 7.867349e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   1/   3][  1000/  4689] |  999/1563 batches | ms/batch 348.0195 | train_loss 1.189408e-01 | lr 7.867349e-05\n",
      "| **** saving model (avg_prec=0) to /tmp/tmpiozofoci at global_step 1000 ****\n",
      "INFO:pecos.xmc.xtransformer.matcher:| **** saving model (avg_prec=0) to /tmp/tmpiozofoci at global_step 1000 ****\n",
      "-----------------------------------------------------------------------------------------\n",
      "INFO:pecos.xmc.xtransformer.matcher:-----------------------------------------------------------------------------------------\n",
      "| [   1/   3][  1100/  4689] | 1099/1563 batches | ms/batch 333.0979 | train_loss 1.235988e-01 | lr 7.654084e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   1/   3][  1100/  4689] | 1099/1563 batches | ms/batch 333.0979 | train_loss 1.235988e-01 | lr 7.654084e-05\n",
      "| [   1/   3][  1200/  4689] | 1199/1563 batches | ms/batch 339.1835 | train_loss 1.190589e-01 | lr 7.440819e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   1/   3][  1200/  4689] | 1199/1563 batches | ms/batch 339.1835 | train_loss 1.190589e-01 | lr 7.440819e-05\n",
      "| [   1/   3][  1300/  4689] | 1299/1563 batches | ms/batch 330.9426 | train_loss 1.167882e-01 | lr 7.227554e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   1/   3][  1300/  4689] | 1299/1563 batches | ms/batch 330.9426 | train_loss 1.167882e-01 | lr 7.227554e-05\n",
      "| [   1/   3][  1400/  4689] | 1399/1563 batches | ms/batch 353.7336 | train_loss 1.205198e-01 | lr 7.014289e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   1/   3][  1400/  4689] | 1399/1563 batches | ms/batch 353.7336 | train_loss 1.205198e-01 | lr 7.014289e-05\n",
      "| [   1/   3][  1500/  4689] | 1499/1563 batches | ms/batch 338.0847 | train_loss 1.165615e-01 | lr 6.801024e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   1/   3][  1500/  4689] | 1499/1563 batches | ms/batch 338.0847 | train_loss 1.165615e-01 | lr 6.801024e-05\n",
      "| [   2/   3][  1600/  4689] |   36/1563 batches | ms/batch 331.0156 | train_loss 1.207707e-01 | lr 6.587759e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  1600/  4689] |   36/1563 batches | ms/batch 331.0156 | train_loss 1.207707e-01 | lr 6.587759e-05\n",
      "| [   2/   3][  1700/  4689] |  136/1563 batches | ms/batch 332.4282 | train_loss 1.008719e-01 | lr 6.374493e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  1700/  4689] |  136/1563 batches | ms/batch 332.4282 | train_loss 1.008719e-01 | lr 6.374493e-05\n",
      "| [   2/   3][  1800/  4689] |  236/1563 batches | ms/batch 359.7387 | train_loss 1.083042e-01 | lr 6.161228e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  1800/  4689] |  236/1563 batches | ms/batch 359.7387 | train_loss 1.083042e-01 | lr 6.161228e-05\n",
      "| [   2/   3][  1900/  4689] |  336/1563 batches | ms/batch 370.4469 | train_loss 1.056862e-01 | lr 5.947963e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  1900/  4689] |  336/1563 batches | ms/batch 370.4469 | train_loss 1.056862e-01 | lr 5.947963e-05\n",
      "| [   2/   3][  2000/  4689] |  436/1563 batches | ms/batch 365.5230 | train_loss 1.042788e-01 | lr 5.734698e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  2000/  4689] |  436/1563 batches | ms/batch 365.5230 | train_loss 1.042788e-01 | lr 5.734698e-05\n",
      "| **** saving model (avg_prec=0) to /tmp/tmpiozofoci at global_step 2000 ****\n",
      "INFO:pecos.xmc.xtransformer.matcher:| **** saving model (avg_prec=0) to /tmp/tmpiozofoci at global_step 2000 ****\n",
      "-----------------------------------------------------------------------------------------\n",
      "INFO:pecos.xmc.xtransformer.matcher:-----------------------------------------------------------------------------------------\n",
      "| [   2/   3][  2100/  4689] |  536/1563 batches | ms/batch 371.8248 | train_loss 1.091191e-01 | lr 5.521433e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  2100/  4689] |  536/1563 batches | ms/batch 371.8248 | train_loss 1.091191e-01 | lr 5.521433e-05\n",
      "| [   2/   3][  2200/  4689] |  636/1563 batches | ms/batch 374.6278 | train_loss 1.098506e-01 | lr 5.308168e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  2200/  4689] |  636/1563 batches | ms/batch 374.6278 | train_loss 1.098506e-01 | lr 5.308168e-05\n",
      "| [   2/   3][  2300/  4689] |  736/1563 batches | ms/batch 381.9465 | train_loss 1.092599e-01 | lr 5.094903e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  2300/  4689] |  736/1563 batches | ms/batch 381.9465 | train_loss 1.092599e-01 | lr 5.094903e-05\n",
      "| [   2/   3][  2400/  4689] |  836/1563 batches | ms/batch 364.8854 | train_loss 1.087893e-01 | lr 4.881638e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  2400/  4689] |  836/1563 batches | ms/batch 364.8854 | train_loss 1.087893e-01 | lr 4.881638e-05\n",
      "| [   2/   3][  2500/  4689] |  936/1563 batches | ms/batch 355.9949 | train_loss 1.099773e-01 | lr 4.668373e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  2500/  4689] |  936/1563 batches | ms/batch 355.9949 | train_loss 1.099773e-01 | lr 4.668373e-05\n",
      "| [   2/   3][  2600/  4689] | 1036/1563 batches | ms/batch 355.7367 | train_loss 1.066235e-01 | lr 4.455108e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  2600/  4689] | 1036/1563 batches | ms/batch 355.7367 | train_loss 1.066235e-01 | lr 4.455108e-05\n",
      "| [   2/   3][  2700/  4689] | 1136/1563 batches | ms/batch 367.8115 | train_loss 1.021171e-01 | lr 4.241843e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  2700/  4689] | 1136/1563 batches | ms/batch 367.8115 | train_loss 1.021171e-01 | lr 4.241843e-05\n",
      "| [   2/   3][  2800/  4689] | 1236/1563 batches | ms/batch 358.6731 | train_loss 1.014165e-01 | lr 4.028578e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  2800/  4689] | 1236/1563 batches | ms/batch 358.6731 | train_loss 1.014165e-01 | lr 4.028578e-05\n",
      "| [   2/   3][  2900/  4689] | 1336/1563 batches | ms/batch 367.8221 | train_loss 9.905572e-02 | lr 3.815312e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  2900/  4689] | 1336/1563 batches | ms/batch 367.8221 | train_loss 9.905572e-02 | lr 3.815312e-05\n",
      "| [   2/   3][  3000/  4689] | 1436/1563 batches | ms/batch 355.8386 | train_loss 9.838734e-02 | lr 3.602047e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  3000/  4689] | 1436/1563 batches | ms/batch 355.8386 | train_loss 9.838734e-02 | lr 3.602047e-05\n",
      "| **** saving model (avg_prec=0) to /tmp/tmpiozofoci at global_step 3000 ****\n",
      "INFO:pecos.xmc.xtransformer.matcher:| **** saving model (avg_prec=0) to /tmp/tmpiozofoci at global_step 3000 ****\n",
      "-----------------------------------------------------------------------------------------\n",
      "INFO:pecos.xmc.xtransformer.matcher:-----------------------------------------------------------------------------------------\n",
      "| [   2/   3][  3100/  4689] | 1536/1563 batches | ms/batch 365.9273 | train_loss 9.257261e-02 | lr 3.388782e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  3100/  4689] | 1536/1563 batches | ms/batch 365.9273 | train_loss 9.257261e-02 | lr 3.388782e-05\n",
      "| [   3/   3][  3200/  4689] |   73/1563 batches | ms/batch 386.2005 | train_loss 9.185906e-02 | lr 3.175517e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   3/   3][  3200/  4689] |   73/1563 batches | ms/batch 386.2005 | train_loss 9.185906e-02 | lr 3.175517e-05\n",
      "| [   3/   3][  3300/  4689] |  173/1563 batches | ms/batch 442.5724 | train_loss 7.959240e-02 | lr 2.962252e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   3/   3][  3300/  4689] |  173/1563 batches | ms/batch 442.5724 | train_loss 7.959240e-02 | lr 2.962252e-05\n",
      "| [   3/   3][  3400/  4689] |  273/1563 batches | ms/batch 414.6878 | train_loss 8.490642e-02 | lr 2.748987e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   3/   3][  3400/  4689] |  273/1563 batches | ms/batch 414.6878 | train_loss 8.490642e-02 | lr 2.748987e-05\n",
      "| [   3/   3][  3500/  4689] |  373/1563 batches | ms/batch 419.1589 | train_loss 7.656555e-02 | lr 2.535722e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   3/   3][  3500/  4689] |  373/1563 batches | ms/batch 419.1589 | train_loss 7.656555e-02 | lr 2.535722e-05\n",
      "| [   3/   3][  3600/  4689] |  473/1563 batches | ms/batch 415.5555 | train_loss 8.168473e-02 | lr 2.322457e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   3/   3][  3600/  4689] |  473/1563 batches | ms/batch 415.5555 | train_loss 8.168473e-02 | lr 2.322457e-05\n",
      "| [   3/   3][  3700/  4689] |  573/1563 batches | ms/batch 370.0086 | train_loss 7.787090e-02 | lr 2.109192e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   3/   3][  3700/  4689] |  573/1563 batches | ms/batch 370.0086 | train_loss 7.787090e-02 | lr 2.109192e-05\n",
      "| [   3/   3][  3800/  4689] |  673/1563 batches | ms/batch 409.5424 | train_loss 7.785695e-02 | lr 1.895927e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   3/   3][  3800/  4689] |  673/1563 batches | ms/batch 409.5424 | train_loss 7.785695e-02 | lr 1.895927e-05\n",
      "| [   3/   3][  3900/  4689] |  773/1563 batches | ms/batch 392.3524 | train_loss 8.315137e-02 | lr 1.682662e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   3/   3][  3900/  4689] |  773/1563 batches | ms/batch 392.3524 | train_loss 8.315137e-02 | lr 1.682662e-05\n",
      "| [   3/   3][  4000/  4689] |  873/1563 batches | ms/batch 356.0942 | train_loss 7.554543e-02 | lr 1.469396e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   3/   3][  4000/  4689] |  873/1563 batches | ms/batch 356.0942 | train_loss 7.554543e-02 | lr 1.469396e-05\n",
      "| **** saving model (avg_prec=0) to /tmp/tmpiozofoci at global_step 4000 ****\n",
      "INFO:pecos.xmc.xtransformer.matcher:| **** saving model (avg_prec=0) to /tmp/tmpiozofoci at global_step 4000 ****\n",
      "-----------------------------------------------------------------------------------------\n",
      "INFO:pecos.xmc.xtransformer.matcher:-----------------------------------------------------------------------------------------\n",
      "| [   3/   3][  4100/  4689] |  973/1563 batches | ms/batch 359.3682 | train_loss 8.125057e-02 | lr 1.256131e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   3/   3][  4100/  4689] |  973/1563 batches | ms/batch 359.3682 | train_loss 8.125057e-02 | lr 1.256131e-05\n",
      "| [   3/   3][  4200/  4689] | 1073/1563 batches | ms/batch 358.6413 | train_loss 7.940089e-02 | lr 1.042866e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   3/   3][  4200/  4689] | 1073/1563 batches | ms/batch 358.6413 | train_loss 7.940089e-02 | lr 1.042866e-05\n",
      "| [   3/   3][  4300/  4689] | 1173/1563 batches | ms/batch 358.6222 | train_loss 8.223188e-02 | lr 8.296012e-06\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   3/   3][  4300/  4689] | 1173/1563 batches | ms/batch 358.6222 | train_loss 8.223188e-02 | lr 8.296012e-06\n",
      "| [   3/   3][  4400/  4689] | 1273/1563 batches | ms/batch 359.3206 | train_loss 7.744880e-02 | lr 6.163361e-06\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   3/   3][  4400/  4689] | 1273/1563 batches | ms/batch 359.3206 | train_loss 7.744880e-02 | lr 6.163361e-06\n",
      "| [   3/   3][  4500/  4689] | 1373/1563 batches | ms/batch 358.9499 | train_loss 7.568107e-02 | lr 4.030710e-06\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   3/   3][  4500/  4689] | 1373/1563 batches | ms/batch 358.9499 | train_loss 7.568107e-02 | lr 4.030710e-06\n",
      "| [   3/   3][  4600/  4689] | 1473/1563 batches | ms/batch 359.1642 | train_loss 7.342247e-02 | lr 1.898059e-06\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   3/   3][  4600/  4689] | 1473/1563 batches | ms/batch 359.1642 | train_loss 7.342247e-02 | lr 1.898059e-06\n",
      "Reload the best checkpoint from /tmp/tmpiozofoci\n",
      "INFO:pecos.xmc.xtransformer.matcher:Reload the best checkpoint from /tmp/tmpiozofoci\n",
      "Predict on input text tensors(torch.Size([50000, 128])) in OVA mode\n",
      "INFO:pecos.xmc.xtransformer.matcher:Predict on input text tensors(torch.Size([50000, 128])) in OVA mode\n",
      "Downloaded distilbert-base-multilingual-cased model from s3.\n",
      "INFO:pecos.xmc.xtransformer.matcher:Downloaded distilbert-base-multilingual-cased model from s3.\n",
      "trn tensors loaded_from /tmp/tmpj13lyfc5/X_trn.pt\n",
      "INFO:pecos.xmc.xtransformer.matcher:trn tensors loaded_from /tmp/tmpj13lyfc5/X_trn.pt\n",
      "Continue training form given text_encoder!\n",
      "INFO:pecos.xmc.xtransformer.matcher:Continue training form given text_encoder!\n",
      "Initialized transformer text_model with xlinear!\n",
      "INFO:pecos.xmc.xtransformer.matcher:Initialized transformer text_model with xlinear!\n",
      "Start fine-tuning transformer matcher...\n",
      "INFO:pecos.xmc.xtransformer.matcher:Start fine-tuning transformer matcher...\n",
      "***** Running training *****\n",
      "INFO:pecos.xmc.xtransformer.matcher:***** Running training *****\n",
      "  Num examples = 50000\n",
      "INFO:pecos.xmc.xtransformer.matcher:  Num examples = 50000\n",
      "  Num labels = 32\n",
      "INFO:pecos.xmc.xtransformer.matcher:  Num labels = 32\n",
      "  Num active labels per instance = 32\n",
      "INFO:pecos.xmc.xtransformer.matcher:  Num active labels per instance = 32\n",
      "  Num Epochs = 3\n",
      "INFO:pecos.xmc.xtransformer.matcher:  Num Epochs = 3\n",
      "  Learning Rate Schedule = linear\n",
      "INFO:pecos.xmc.xtransformer.matcher:  Learning Rate Schedule = linear\n",
      "  Batch size = 32\n",
      "INFO:pecos.xmc.xtransformer.matcher:  Batch size = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "INFO:pecos.xmc.xtransformer.matcher:  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4689\n",
      "INFO:pecos.xmc.xtransformer.matcher:  Total optimization steps = 4689\n",
      "| [   1/   3][   100/  4689] |   99/1563 batches | ms/batch 313.4872 | train_loss 6.627106e-01 | lr 9.786735e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   1/   3][   100/  4689] |   99/1563 batches | ms/batch 313.4872 | train_loss 6.627106e-01 | lr 9.786735e-05\n",
      "| [   1/   3][   200/  4689] |  199/1563 batches | ms/batch 333.2811 | train_loss 5.912123e-01 | lr 9.573470e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   1/   3][   200/  4689] |  199/1563 batches | ms/batch 333.2811 | train_loss 5.912123e-01 | lr 9.573470e-05\n",
      "| [   1/   3][   300/  4689] |  299/1563 batches | ms/batch 333.3999 | train_loss 5.751077e-01 | lr 9.360205e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   1/   3][   300/  4689] |  299/1563 batches | ms/batch 333.3999 | train_loss 5.751077e-01 | lr 9.360205e-05\n",
      "| [   1/   3][   400/  4689] |  399/1563 batches | ms/batch 318.6428 | train_loss 5.640469e-01 | lr 9.146940e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   1/   3][   400/  4689] |  399/1563 batches | ms/batch 318.6428 | train_loss 5.640469e-01 | lr 9.146940e-05\n",
      "| [   1/   3][   500/  4689] |  499/1563 batches | ms/batch 316.0100 | train_loss 5.634147e-01 | lr 8.933675e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   1/   3][   500/  4689] |  499/1563 batches | ms/batch 316.0100 | train_loss 5.634147e-01 | lr 8.933675e-05\n",
      "| [   1/   3][   600/  4689] |  599/1563 batches | ms/batch 351.6266 | train_loss 5.616846e-01 | lr 8.720409e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   1/   3][   600/  4689] |  599/1563 batches | ms/batch 351.6266 | train_loss 5.616846e-01 | lr 8.720409e-05\n",
      "| [   1/   3][   700/  4689] |  699/1563 batches | ms/batch 334.1447 | train_loss 5.563698e-01 | lr 8.507144e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   1/   3][   700/  4689] |  699/1563 batches | ms/batch 334.1447 | train_loss 5.563698e-01 | lr 8.507144e-05\n",
      "| [   1/   3][   800/  4689] |  799/1563 batches | ms/batch 314.5882 | train_loss 5.546068e-01 | lr 8.293879e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   1/   3][   800/  4689] |  799/1563 batches | ms/batch 314.5882 | train_loss 5.546068e-01 | lr 8.293879e-05\n",
      "| [   1/   3][   900/  4689] |  899/1563 batches | ms/batch 317.4052 | train_loss 5.517618e-01 | lr 8.080614e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   1/   3][   900/  4689] |  899/1563 batches | ms/batch 317.4052 | train_loss 5.517618e-01 | lr 8.080614e-05\n",
      "| [   1/   3][  1000/  4689] |  999/1563 batches | ms/batch 307.5022 | train_loss 5.545211e-01 | lr 7.867349e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   1/   3][  1000/  4689] |  999/1563 batches | ms/batch 307.5022 | train_loss 5.545211e-01 | lr 7.867349e-05\n",
      "| **** saving model (avg_prec=0) to /tmp/tmpzm_qgjnh at global_step 1000 ****\n",
      "INFO:pecos.xmc.xtransformer.matcher:| **** saving model (avg_prec=0) to /tmp/tmpzm_qgjnh at global_step 1000 ****\n",
      "-----------------------------------------------------------------------------------------\n",
      "INFO:pecos.xmc.xtransformer.matcher:-----------------------------------------------------------------------------------------\n",
      "| [   1/   3][  1100/  4689] | 1099/1563 batches | ms/batch 307.6803 | train_loss 5.540994e-01 | lr 7.654084e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   1/   3][  1100/  4689] | 1099/1563 batches | ms/batch 307.6803 | train_loss 5.540994e-01 | lr 7.654084e-05\n",
      "| [   1/   3][  1200/  4689] | 1199/1563 batches | ms/batch 313.5600 | train_loss 5.486660e-01 | lr 7.440819e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   1/   3][  1200/  4689] | 1199/1563 batches | ms/batch 313.5600 | train_loss 5.486660e-01 | lr 7.440819e-05\n",
      "| [   1/   3][  1300/  4689] | 1299/1563 batches | ms/batch 309.5855 | train_loss 5.501738e-01 | lr 7.227554e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   1/   3][  1300/  4689] | 1299/1563 batches | ms/batch 309.5855 | train_loss 5.501738e-01 | lr 7.227554e-05\n",
      "| [   1/   3][  1400/  4689] | 1399/1563 batches | ms/batch 298.3118 | train_loss 5.498195e-01 | lr 7.014289e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   1/   3][  1400/  4689] | 1399/1563 batches | ms/batch 298.3118 | train_loss 5.498195e-01 | lr 7.014289e-05\n",
      "| [   1/   3][  1500/  4689] | 1499/1563 batches | ms/batch 297.1050 | train_loss 5.483910e-01 | lr 6.801024e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   1/   3][  1500/  4689] | 1499/1563 batches | ms/batch 297.1050 | train_loss 5.483910e-01 | lr 6.801024e-05\n",
      "| [   2/   3][  1600/  4689] |   36/1563 batches | ms/batch 319.3975 | train_loss 5.527303e-01 | lr 6.587759e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  1600/  4689] |   36/1563 batches | ms/batch 319.3975 | train_loss 5.527303e-01 | lr 6.587759e-05\n",
      "| [   2/   3][  1700/  4689] |  136/1563 batches | ms/batch 318.4491 | train_loss 5.406213e-01 | lr 6.374493e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  1700/  4689] |  136/1563 batches | ms/batch 318.4491 | train_loss 5.406213e-01 | lr 6.374493e-05\n",
      "| [   2/   3][  1800/  4689] |  236/1563 batches | ms/batch 341.2580 | train_loss 5.470897e-01 | lr 6.161228e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  1800/  4689] |  236/1563 batches | ms/batch 341.2580 | train_loss 5.470897e-01 | lr 6.161228e-05\n",
      "| [   2/   3][  1900/  4689] |  336/1563 batches | ms/batch 331.6004 | train_loss 5.453250e-01 | lr 5.947963e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  1900/  4689] |  336/1563 batches | ms/batch 331.6004 | train_loss 5.453250e-01 | lr 5.947963e-05\n",
      "| [   2/   3][  2000/  4689] |  436/1563 batches | ms/batch 330.5268 | train_loss 5.416475e-01 | lr 5.734698e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  2000/  4689] |  436/1563 batches | ms/batch 330.5268 | train_loss 5.416475e-01 | lr 5.734698e-05\n",
      "| **** saving model (avg_prec=0) to /tmp/tmpzm_qgjnh at global_step 2000 ****\n",
      "INFO:pecos.xmc.xtransformer.matcher:| **** saving model (avg_prec=0) to /tmp/tmpzm_qgjnh at global_step 2000 ****\n",
      "-----------------------------------------------------------------------------------------\n",
      "INFO:pecos.xmc.xtransformer.matcher:-----------------------------------------------------------------------------------------\n",
      "| [   2/   3][  2100/  4689] |  536/1563 batches | ms/batch 314.5760 | train_loss 5.431218e-01 | lr 5.521433e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  2100/  4689] |  536/1563 batches | ms/batch 314.5760 | train_loss 5.431218e-01 | lr 5.521433e-05\n",
      "| [   2/   3][  2200/  4689] |  636/1563 batches | ms/batch 328.7454 | train_loss 5.422998e-01 | lr 5.308168e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  2200/  4689] |  636/1563 batches | ms/batch 328.7454 | train_loss 5.422998e-01 | lr 5.308168e-05\n",
      "| [   2/   3][  2300/  4689] |  736/1563 batches | ms/batch 322.6053 | train_loss 5.432600e-01 | lr 5.094903e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  2300/  4689] |  736/1563 batches | ms/batch 322.6053 | train_loss 5.432600e-01 | lr 5.094903e-05\n",
      "| [   2/   3][  2400/  4689] |  836/1563 batches | ms/batch 316.7511 | train_loss 5.422008e-01 | lr 4.881638e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  2400/  4689] |  836/1563 batches | ms/batch 316.7511 | train_loss 5.422008e-01 | lr 4.881638e-05\n",
      "| [   2/   3][  2500/  4689] |  936/1563 batches | ms/batch 317.6355 | train_loss 5.399661e-01 | lr 4.668373e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  2500/  4689] |  936/1563 batches | ms/batch 317.6355 | train_loss 5.399661e-01 | lr 4.668373e-05\n",
      "| [   2/   3][  2600/  4689] | 1036/1563 batches | ms/batch 298.3759 | train_loss 5.397037e-01 | lr 4.455108e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  2600/  4689] | 1036/1563 batches | ms/batch 298.3759 | train_loss 5.397037e-01 | lr 4.455108e-05\n",
      "| [   2/   3][  2700/  4689] | 1136/1563 batches | ms/batch 316.1346 | train_loss 5.416176e-01 | lr 4.241843e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  2700/  4689] | 1136/1563 batches | ms/batch 316.1346 | train_loss 5.416176e-01 | lr 4.241843e-05\n",
      "| [   2/   3][  2800/  4689] | 1236/1563 batches | ms/batch 349.6473 | train_loss 5.421170e-01 | lr 4.028578e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  2800/  4689] | 1236/1563 batches | ms/batch 349.6473 | train_loss 5.421170e-01 | lr 4.028578e-05\n",
      "| [   2/   3][  2900/  4689] | 1336/1563 batches | ms/batch 326.3884 | train_loss 5.388009e-01 | lr 3.815312e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  2900/  4689] | 1336/1563 batches | ms/batch 326.3884 | train_loss 5.388009e-01 | lr 3.815312e-05\n",
      "| [   2/   3][  3000/  4689] | 1436/1563 batches | ms/batch 318.1683 | train_loss 5.369682e-01 | lr 3.602047e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  3000/  4689] | 1436/1563 batches | ms/batch 318.1683 | train_loss 5.369682e-01 | lr 3.602047e-05\n",
      "| **** saving model (avg_prec=0) to /tmp/tmpzm_qgjnh at global_step 3000 ****\n",
      "INFO:pecos.xmc.xtransformer.matcher:| **** saving model (avg_prec=0) to /tmp/tmpzm_qgjnh at global_step 3000 ****\n",
      "-----------------------------------------------------------------------------------------\n",
      "INFO:pecos.xmc.xtransformer.matcher:-----------------------------------------------------------------------------------------\n",
      "| [   2/   3][  3100/  4689] | 1536/1563 batches | ms/batch 321.2811 | train_loss 5.382942e-01 | lr 3.388782e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  3100/  4689] | 1536/1563 batches | ms/batch 321.2811 | train_loss 5.382942e-01 | lr 3.388782e-05\n",
      "| [   3/   3][  3200/  4689] |   73/1563 batches | ms/batch 327.1579 | train_loss 5.367007e-01 | lr 3.175517e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   3/   3][  3200/  4689] |   73/1563 batches | ms/batch 327.1579 | train_loss 5.367007e-01 | lr 3.175517e-05\n",
      "| [   3/   3][  3300/  4689] |  173/1563 batches | ms/batch 336.2868 | train_loss 5.313147e-01 | lr 2.962252e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   3/   3][  3300/  4689] |  173/1563 batches | ms/batch 336.2868 | train_loss 5.313147e-01 | lr 2.962252e-05\n",
      "| [   3/   3][  3400/  4689] |  273/1563 batches | ms/batch 331.4797 | train_loss 5.334346e-01 | lr 2.748987e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   3/   3][  3400/  4689] |  273/1563 batches | ms/batch 331.4797 | train_loss 5.334346e-01 | lr 2.748987e-05\n",
      "| [   3/   3][  3500/  4689] |  373/1563 batches | ms/batch 326.6209 | train_loss 5.303996e-01 | lr 2.535722e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   3/   3][  3500/  4689] |  373/1563 batches | ms/batch 326.6209 | train_loss 5.303996e-01 | lr 2.535722e-05\n",
      "| [   3/   3][  3600/  4689] |  473/1563 batches | ms/batch 343.3229 | train_loss 5.315787e-01 | lr 2.322457e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   3/   3][  3600/  4689] |  473/1563 batches | ms/batch 343.3229 | train_loss 5.315787e-01 | lr 2.322457e-05\n",
      "| [   3/   3][  3700/  4689] |  573/1563 batches | ms/batch 343.0813 | train_loss 5.325044e-01 | lr 2.109192e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   3/   3][  3700/  4689] |  573/1563 batches | ms/batch 343.0813 | train_loss 5.325044e-01 | lr 2.109192e-05\n",
      "| [   3/   3][  3800/  4689] |  673/1563 batches | ms/batch 328.4249 | train_loss 5.285486e-01 | lr 1.895927e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   3/   3][  3800/  4689] |  673/1563 batches | ms/batch 328.4249 | train_loss 5.285486e-01 | lr 1.895927e-05\n",
      "| [   3/   3][  3900/  4689] |  773/1563 batches | ms/batch 311.4439 | train_loss 5.320386e-01 | lr 1.682662e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   3/   3][  3900/  4689] |  773/1563 batches | ms/batch 311.4439 | train_loss 5.320386e-01 | lr 1.682662e-05\n",
      "| [   3/   3][  4000/  4689] |  873/1563 batches | ms/batch 325.8076 | train_loss 5.316801e-01 | lr 1.469396e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   3/   3][  4000/  4689] |  873/1563 batches | ms/batch 325.8076 | train_loss 5.316801e-01 | lr 1.469396e-05\n",
      "| **** saving model (avg_prec=0) to /tmp/tmpzm_qgjnh at global_step 4000 ****\n",
      "INFO:pecos.xmc.xtransformer.matcher:| **** saving model (avg_prec=0) to /tmp/tmpzm_qgjnh at global_step 4000 ****\n",
      "-----------------------------------------------------------------------------------------\n",
      "INFO:pecos.xmc.xtransformer.matcher:-----------------------------------------------------------------------------------------\n",
      "| [   3/   3][  4100/  4689] |  973/1563 batches | ms/batch 314.6383 | train_loss 5.337967e-01 | lr 1.256131e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   3/   3][  4100/  4689] |  973/1563 batches | ms/batch 314.6383 | train_loss 5.337967e-01 | lr 1.256131e-05\n",
      "| [   3/   3][  4200/  4689] | 1073/1563 batches | ms/batch 323.2132 | train_loss 5.289830e-01 | lr 1.042866e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   3/   3][  4200/  4689] | 1073/1563 batches | ms/batch 323.2132 | train_loss 5.289830e-01 | lr 1.042866e-05\n",
      "| [   3/   3][  4300/  4689] | 1173/1563 batches | ms/batch 321.0773 | train_loss 5.319513e-01 | lr 8.296012e-06\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   3/   3][  4300/  4689] | 1173/1563 batches | ms/batch 321.0773 | train_loss 5.319513e-01 | lr 8.296012e-06\n",
      "| [   3/   3][  4400/  4689] | 1273/1563 batches | ms/batch 331.1271 | train_loss 5.372517e-01 | lr 6.163361e-06\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   3/   3][  4400/  4689] | 1273/1563 batches | ms/batch 331.1271 | train_loss 5.372517e-01 | lr 6.163361e-06\n",
      "| [   3/   3][  4500/  4689] | 1373/1563 batches | ms/batch 322.1515 | train_loss 5.323522e-01 | lr 4.030710e-06\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   3/   3][  4500/  4689] | 1373/1563 batches | ms/batch 322.1515 | train_loss 5.323522e-01 | lr 4.030710e-06\n",
      "| [   3/   3][  4600/  4689] | 1473/1563 batches | ms/batch 341.7665 | train_loss 5.299909e-01 | lr 1.898059e-06\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   3/   3][  4600/  4689] | 1473/1563 batches | ms/batch 341.7665 | train_loss 5.299909e-01 | lr 1.898059e-06\n",
      "Reload the best checkpoint from /tmp/tmpzm_qgjnh\n",
      "INFO:pecos.xmc.xtransformer.matcher:Reload the best checkpoint from /tmp/tmpzm_qgjnh\n",
      "Predict with csr_codes_next((50000, 32)) with avr_nnz=32.0\n",
      "INFO:pecos.xmc.xtransformer.matcher:Predict with csr_codes_next((50000, 32)) with avr_nnz=32.0\n",
      "Downloaded distilbert-base-multilingual-cased model from s3.\n",
      "INFO:pecos.xmc.xtransformer.matcher:Downloaded distilbert-base-multilingual-cased model from s3.\n",
      "trn tensors loaded_from /tmp/tmpj13lyfc5/X_trn.pt\n",
      "INFO:pecos.xmc.xtransformer.matcher:trn tensors loaded_from /tmp/tmpj13lyfc5/X_trn.pt\n",
      "Continue training form given text_encoder!\n",
      "INFO:pecos.xmc.xtransformer.matcher:Continue training form given text_encoder!\n",
      "Initialized transformer text_model with xlinear!\n",
      "INFO:pecos.xmc.xtransformer.matcher:Initialized transformer text_model with xlinear!\n",
      "Start fine-tuning transformer matcher...\n",
      "INFO:pecos.xmc.xtransformer.matcher:Start fine-tuning transformer matcher...\n",
      "***** Running training *****\n",
      "INFO:pecos.xmc.xtransformer.matcher:***** Running training *****\n",
      "  Num examples = 50000\n",
      "INFO:pecos.xmc.xtransformer.matcher:  Num examples = 50000\n",
      "  Num labels = 512\n",
      "INFO:pecos.xmc.xtransformer.matcher:  Num labels = 512\n",
      "  Num active labels per instance = 128\n",
      "INFO:pecos.xmc.xtransformer.matcher:  Num active labels per instance = 128\n",
      "  Num Epochs = 3\n",
      "INFO:pecos.xmc.xtransformer.matcher:  Num Epochs = 3\n",
      "  Learning Rate Schedule = linear\n",
      "INFO:pecos.xmc.xtransformer.matcher:  Learning Rate Schedule = linear\n",
      "  Batch size = 32\n",
      "INFO:pecos.xmc.xtransformer.matcher:  Batch size = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "INFO:pecos.xmc.xtransformer.matcher:  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4689\n",
      "INFO:pecos.xmc.xtransformer.matcher:  Total optimization steps = 4689\n",
      "| [   1/   3][   100/  4689] |   99/1563 batches | ms/batch 345.2333 | train_loss 1.056578e+00 | lr 9.786735e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   1/   3][   100/  4689] |   99/1563 batches | ms/batch 345.2333 | train_loss 1.056578e+00 | lr 9.786735e-05\n",
      "| [   1/   3][   200/  4689] |  199/1563 batches | ms/batch 370.4545 | train_loss 9.323169e-01 | lr 9.573470e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   1/   3][   200/  4689] |  199/1563 batches | ms/batch 370.4545 | train_loss 9.323169e-01 | lr 9.573470e-05\n",
      "| [   1/   3][   300/  4689] |  299/1563 batches | ms/batch 352.0852 | train_loss 9.261903e-01 | lr 9.360205e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   1/   3][   300/  4689] |  299/1563 batches | ms/batch 352.0852 | train_loss 9.261903e-01 | lr 9.360205e-05\n",
      "| [   1/   3][   400/  4689] |  399/1563 batches | ms/batch 339.4575 | train_loss 9.148487e-01 | lr 9.146940e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   1/   3][   400/  4689] |  399/1563 batches | ms/batch 339.4575 | train_loss 9.148487e-01 | lr 9.146940e-05\n",
      "| [   1/   3][   500/  4689] |  499/1563 batches | ms/batch 333.2420 | train_loss 9.060487e-01 | lr 8.933675e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   1/   3][   500/  4689] |  499/1563 batches | ms/batch 333.2420 | train_loss 9.060487e-01 | lr 8.933675e-05\n",
      "| [   1/   3][   600/  4689] |  599/1563 batches | ms/batch 333.9463 | train_loss 8.961763e-01 | lr 8.720409e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   1/   3][   600/  4689] |  599/1563 batches | ms/batch 333.9463 | train_loss 8.961763e-01 | lr 8.720409e-05\n",
      "| [   1/   3][   700/  4689] |  699/1563 batches | ms/batch 332.3194 | train_loss 8.904768e-01 | lr 8.507144e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   1/   3][   700/  4689] |  699/1563 batches | ms/batch 332.3194 | train_loss 8.904768e-01 | lr 8.507144e-05\n",
      "| [   1/   3][   800/  4689] |  799/1563 batches | ms/batch 322.1421 | train_loss 8.850014e-01 | lr 8.293879e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   1/   3][   800/  4689] |  799/1563 batches | ms/batch 322.1421 | train_loss 8.850014e-01 | lr 8.293879e-05\n",
      "| [   1/   3][   900/  4689] |  899/1563 batches | ms/batch 315.5715 | train_loss 8.844527e-01 | lr 8.080614e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   1/   3][   900/  4689] |  899/1563 batches | ms/batch 315.5715 | train_loss 8.844527e-01 | lr 8.080614e-05\n",
      "| [   1/   3][  1000/  4689] |  999/1563 batches | ms/batch 329.5181 | train_loss 8.769778e-01 | lr 7.867349e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   1/   3][  1000/  4689] |  999/1563 batches | ms/batch 329.5181 | train_loss 8.769778e-01 | lr 7.867349e-05\n",
      "| **** saving model (avg_prec=0) to /tmp/tmp_ioibtna at global_step 1000 ****\n",
      "INFO:pecos.xmc.xtransformer.matcher:| **** saving model (avg_prec=0) to /tmp/tmp_ioibtna at global_step 1000 ****\n",
      "-----------------------------------------------------------------------------------------\n",
      "INFO:pecos.xmc.xtransformer.matcher:-----------------------------------------------------------------------------------------\n",
      "| [   1/   3][  1100/  4689] | 1099/1563 batches | ms/batch 320.8197 | train_loss 8.773802e-01 | lr 7.654084e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   1/   3][  1100/  4689] | 1099/1563 batches | ms/batch 320.8197 | train_loss 8.773802e-01 | lr 7.654084e-05\n",
      "| [   1/   3][  1200/  4689] | 1199/1563 batches | ms/batch 380.0984 | train_loss 8.663944e-01 | lr 7.440819e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   1/   3][  1200/  4689] | 1199/1563 batches | ms/batch 380.0984 | train_loss 8.663944e-01 | lr 7.440819e-05\n",
      "| [   1/   3][  1300/  4689] | 1299/1563 batches | ms/batch 384.2967 | train_loss 8.660203e-01 | lr 7.227554e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   1/   3][  1300/  4689] | 1299/1563 batches | ms/batch 384.2967 | train_loss 8.660203e-01 | lr 7.227554e-05\n",
      "| [   1/   3][  1400/  4689] | 1399/1563 batches | ms/batch 346.4392 | train_loss 8.676476e-01 | lr 7.014289e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   1/   3][  1400/  4689] | 1399/1563 batches | ms/batch 346.4392 | train_loss 8.676476e-01 | lr 7.014289e-05\n",
      "| [   1/   3][  1500/  4689] | 1499/1563 batches | ms/batch 345.4938 | train_loss 8.608428e-01 | lr 6.801024e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   1/   3][  1500/  4689] | 1499/1563 batches | ms/batch 345.4938 | train_loss 8.608428e-01 | lr 6.801024e-05\n",
      "| [   2/   3][  1600/  4689] |   36/1563 batches | ms/batch 371.7932 | train_loss 8.558803e-01 | lr 6.587759e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  1600/  4689] |   36/1563 batches | ms/batch 371.7932 | train_loss 8.558803e-01 | lr 6.587759e-05\n",
      "| [   2/   3][  1700/  4689] |  136/1563 batches | ms/batch 346.7223 | train_loss 8.557581e-01 | lr 6.374493e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  1700/  4689] |  136/1563 batches | ms/batch 346.7223 | train_loss 8.557581e-01 | lr 6.374493e-05\n",
      "| [   2/   3][  1800/  4689] |  236/1563 batches | ms/batch 359.7193 | train_loss 8.503712e-01 | lr 6.161228e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  1800/  4689] |  236/1563 batches | ms/batch 359.7193 | train_loss 8.503712e-01 | lr 6.161228e-05\n",
      "| [   2/   3][  1900/  4689] |  336/1563 batches | ms/batch 364.8729 | train_loss 8.539621e-01 | lr 5.947963e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  1900/  4689] |  336/1563 batches | ms/batch 364.8729 | train_loss 8.539621e-01 | lr 5.947963e-05\n",
      "| [   2/   3][  2000/  4689] |  436/1563 batches | ms/batch 340.5275 | train_loss 8.504949e-01 | lr 5.734698e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  2000/  4689] |  436/1563 batches | ms/batch 340.5275 | train_loss 8.504949e-01 | lr 5.734698e-05\n",
      "| **** saving model (avg_prec=0) to /tmp/tmp_ioibtna at global_step 2000 ****\n",
      "INFO:pecos.xmc.xtransformer.matcher:| **** saving model (avg_prec=0) to /tmp/tmp_ioibtna at global_step 2000 ****\n",
      "-----------------------------------------------------------------------------------------\n",
      "INFO:pecos.xmc.xtransformer.matcher:-----------------------------------------------------------------------------------------\n",
      "| [   2/   3][  2100/  4689] |  536/1563 batches | ms/batch 339.0503 | train_loss 8.502331e-01 | lr 5.521433e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  2100/  4689] |  536/1563 batches | ms/batch 339.0503 | train_loss 8.502331e-01 | lr 5.521433e-05\n",
      "| [   2/   3][  2200/  4689] |  636/1563 batches | ms/batch 326.1600 | train_loss 8.476233e-01 | lr 5.308168e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  2200/  4689] |  636/1563 batches | ms/batch 326.1600 | train_loss 8.476233e-01 | lr 5.308168e-05\n",
      "| [   2/   3][  2300/  4689] |  736/1563 batches | ms/batch 323.3479 | train_loss 8.484934e-01 | lr 5.094903e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  2300/  4689] |  736/1563 batches | ms/batch 323.3479 | train_loss 8.484934e-01 | lr 5.094903e-05\n",
      "| [   2/   3][  2400/  4689] |  836/1563 batches | ms/batch 326.3933 | train_loss 8.440185e-01 | lr 4.881638e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  2400/  4689] |  836/1563 batches | ms/batch 326.3933 | train_loss 8.440185e-01 | lr 4.881638e-05\n",
      "| [   2/   3][  2500/  4689] |  936/1563 batches | ms/batch 325.1226 | train_loss 8.420463e-01 | lr 4.668373e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  2500/  4689] |  936/1563 batches | ms/batch 325.1226 | train_loss 8.420463e-01 | lr 4.668373e-05\n",
      "| [   2/   3][  2600/  4689] | 1036/1563 batches | ms/batch 325.1527 | train_loss 8.426441e-01 | lr 4.455108e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  2600/  4689] | 1036/1563 batches | ms/batch 325.1527 | train_loss 8.426441e-01 | lr 4.455108e-05\n",
      "| [   2/   3][  2700/  4689] | 1136/1563 batches | ms/batch 329.2598 | train_loss 8.447235e-01 | lr 4.241843e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  2700/  4689] | 1136/1563 batches | ms/batch 329.2598 | train_loss 8.447235e-01 | lr 4.241843e-05\n",
      "| [   2/   3][  2800/  4689] | 1236/1563 batches | ms/batch 328.6173 | train_loss 8.414539e-01 | lr 4.028578e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  2800/  4689] | 1236/1563 batches | ms/batch 328.6173 | train_loss 8.414539e-01 | lr 4.028578e-05\n",
      "| [   2/   3][  2900/  4689] | 1336/1563 batches | ms/batch 319.8491 | train_loss 8.414273e-01 | lr 3.815312e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  2900/  4689] | 1336/1563 batches | ms/batch 319.8491 | train_loss 8.414273e-01 | lr 3.815312e-05\n",
      "| [   2/   3][  3000/  4689] | 1436/1563 batches | ms/batch 312.6618 | train_loss 8.385805e-01 | lr 3.602047e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  3000/  4689] | 1436/1563 batches | ms/batch 312.6618 | train_loss 8.385805e-01 | lr 3.602047e-05\n",
      "| **** saving model (avg_prec=0) to /tmp/tmp_ioibtna at global_step 3000 ****\n",
      "INFO:pecos.xmc.xtransformer.matcher:| **** saving model (avg_prec=0) to /tmp/tmp_ioibtna at global_step 3000 ****\n",
      "-----------------------------------------------------------------------------------------\n",
      "INFO:pecos.xmc.xtransformer.matcher:-----------------------------------------------------------------------------------------\n",
      "| [   2/   3][  3100/  4689] | 1536/1563 batches | ms/batch 319.8292 | train_loss 8.426175e-01 | lr 3.388782e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  3100/  4689] | 1536/1563 batches | ms/batch 319.8292 | train_loss 8.426175e-01 | lr 3.388782e-05\n",
      "| [   3/   3][  3200/  4689] |   73/1563 batches | ms/batch 317.0782 | train_loss 8.407066e-01 | lr 3.175517e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   3/   3][  3200/  4689] |   73/1563 batches | ms/batch 317.0782 | train_loss 8.407066e-01 | lr 3.175517e-05\n",
      "| [   3/   3][  3300/  4689] |  173/1563 batches | ms/batch 313.7250 | train_loss 8.366158e-01 | lr 2.962252e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   3/   3][  3300/  4689] |  173/1563 batches | ms/batch 313.7250 | train_loss 8.366158e-01 | lr 2.962252e-05\n",
      "| [   3/   3][  3400/  4689] |  273/1563 batches | ms/batch 316.5875 | train_loss 8.371202e-01 | lr 2.748987e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   3/   3][  3400/  4689] |  273/1563 batches | ms/batch 316.5875 | train_loss 8.371202e-01 | lr 2.748987e-05\n",
      "| [   3/   3][  3500/  4689] |  373/1563 batches | ms/batch 318.8016 | train_loss 8.330767e-01 | lr 2.535722e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   3/   3][  3500/  4689] |  373/1563 batches | ms/batch 318.8016 | train_loss 8.330767e-01 | lr 2.535722e-05\n",
      "| [   3/   3][  3600/  4689] |  473/1563 batches | ms/batch 325.4189 | train_loss 8.334449e-01 | lr 2.322457e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   3/   3][  3600/  4689] |  473/1563 batches | ms/batch 325.4189 | train_loss 8.334449e-01 | lr 2.322457e-05\n",
      "| [   3/   3][  3700/  4689] |  573/1563 batches | ms/batch 332.9693 | train_loss 8.340112e-01 | lr 2.109192e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   3/   3][  3700/  4689] |  573/1563 batches | ms/batch 332.9693 | train_loss 8.340112e-01 | lr 2.109192e-05\n",
      "| [   3/   3][  3800/  4689] |  673/1563 batches | ms/batch 316.5935 | train_loss 8.330754e-01 | lr 1.895927e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   3/   3][  3800/  4689] |  673/1563 batches | ms/batch 316.5935 | train_loss 8.330754e-01 | lr 1.895927e-05\n",
      "| [   3/   3][  3900/  4689] |  773/1563 batches | ms/batch 314.8873 | train_loss 8.347002e-01 | lr 1.682662e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   3/   3][  3900/  4689] |  773/1563 batches | ms/batch 314.8873 | train_loss 8.347002e-01 | lr 1.682662e-05\n",
      "| [   3/   3][  4000/  4689] |  873/1563 batches | ms/batch 317.8064 | train_loss 8.342190e-01 | lr 1.469396e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   3/   3][  4000/  4689] |  873/1563 batches | ms/batch 317.8064 | train_loss 8.342190e-01 | lr 1.469396e-05\n",
      "| **** saving model (avg_prec=0) to /tmp/tmp_ioibtna at global_step 4000 ****\n",
      "INFO:pecos.xmc.xtransformer.matcher:| **** saving model (avg_prec=0) to /tmp/tmp_ioibtna at global_step 4000 ****\n",
      "-----------------------------------------------------------------------------------------\n",
      "INFO:pecos.xmc.xtransformer.matcher:-----------------------------------------------------------------------------------------\n",
      "| [   3/   3][  4100/  4689] |  973/1563 batches | ms/batch 316.9198 | train_loss 8.320544e-01 | lr 1.256131e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   3/   3][  4100/  4689] |  973/1563 batches | ms/batch 316.9198 | train_loss 8.320544e-01 | lr 1.256131e-05\n",
      "| [   3/   3][  4200/  4689] | 1073/1563 batches | ms/batch 317.2679 | train_loss 8.322918e-01 | lr 1.042866e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   3/   3][  4200/  4689] | 1073/1563 batches | ms/batch 317.2679 | train_loss 8.322918e-01 | lr 1.042866e-05\n",
      "| [   3/   3][  4300/  4689] | 1173/1563 batches | ms/batch 324.6357 | train_loss 8.296397e-01 | lr 8.296012e-06\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   3/   3][  4300/  4689] | 1173/1563 batches | ms/batch 324.6357 | train_loss 8.296397e-01 | lr 8.296012e-06\n",
      "| [   3/   3][  4400/  4689] | 1273/1563 batches | ms/batch 321.2341 | train_loss 8.299162e-01 | lr 6.163361e-06\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   3/   3][  4400/  4689] | 1273/1563 batches | ms/batch 321.2341 | train_loss 8.299162e-01 | lr 6.163361e-06\n",
      "| [   3/   3][  4500/  4689] | 1373/1563 batches | ms/batch 324.3481 | train_loss 8.315721e-01 | lr 4.030710e-06\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   3/   3][  4500/  4689] | 1373/1563 batches | ms/batch 324.3481 | train_loss 8.315721e-01 | lr 4.030710e-06\n",
      "| [   3/   3][  4600/  4689] | 1473/1563 batches | ms/batch 322.1898 | train_loss 8.298767e-01 | lr 1.898059e-06\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   3/   3][  4600/  4689] | 1473/1563 batches | ms/batch 322.1898 | train_loss 8.298767e-01 | lr 1.898059e-06\n",
      "Reload the best checkpoint from /tmp/tmp_ioibtna\n",
      "INFO:pecos.xmc.xtransformer.matcher:Reload the best checkpoint from /tmp/tmp_ioibtna\n",
      "Predict with csr_codes_next((50000, 512)) with avr_nnz=320.0\n",
      "INFO:pecos.xmc.xtransformer.matcher:Predict with csr_codes_next((50000, 512)) with avr_nnz=320.0\n",
      "Downloaded distilbert-base-multilingual-cased model from s3.\n",
      "INFO:pecos.xmc.xtransformer.matcher:Downloaded distilbert-base-multilingual-cased model from s3.\n",
      "trn tensors loaded_from /tmp/tmpj13lyfc5/X_trn.pt\n",
      "INFO:pecos.xmc.xtransformer.matcher:trn tensors loaded_from /tmp/tmpj13lyfc5/X_trn.pt\n",
      "Continue training form given text_encoder!\n",
      "INFO:pecos.xmc.xtransformer.matcher:Continue training form given text_encoder!\n",
      "Initialized transformer text_model with xlinear!\n",
      "INFO:pecos.xmc.xtransformer.matcher:Initialized transformer text_model with xlinear!\n",
      "Start fine-tuning transformer matcher...\n",
      "INFO:pecos.xmc.xtransformer.matcher:Start fine-tuning transformer matcher...\n",
      "***** Running training *****\n",
      "INFO:pecos.xmc.xtransformer.matcher:***** Running training *****\n",
      "  Num examples = 50000\n",
      "INFO:pecos.xmc.xtransformer.matcher:  Num examples = 50000\n",
      "  Num labels = 8192\n",
      "INFO:pecos.xmc.xtransformer.matcher:  Num labels = 8192\n",
      "  Num active labels per instance = 176\n",
      "INFO:pecos.xmc.xtransformer.matcher:  Num active labels per instance = 176\n",
      "  Num Epochs = 3\n",
      "INFO:pecos.xmc.xtransformer.matcher:  Num Epochs = 3\n",
      "  Learning Rate Schedule = linear\n",
      "INFO:pecos.xmc.xtransformer.matcher:  Learning Rate Schedule = linear\n",
      "  Batch size = 32\n",
      "INFO:pecos.xmc.xtransformer.matcher:  Batch size = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "INFO:pecos.xmc.xtransformer.matcher:  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4689\n",
      "INFO:pecos.xmc.xtransformer.matcher:  Total optimization steps = 4689\n",
      "| [   1/   3][   100/  4689] |   99/1563 batches | ms/batch 295.0321 | train_loss 9.416978e-01 | lr 9.786735e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   1/   3][   100/  4689] |   99/1563 batches | ms/batch 295.0321 | train_loss 9.416978e-01 | lr 9.786735e-05\n",
      "| [   1/   3][   200/  4689] |  199/1563 batches | ms/batch 295.5493 | train_loss 8.877688e-01 | lr 9.573470e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   1/   3][   200/  4689] |  199/1563 batches | ms/batch 295.5493 | train_loss 8.877688e-01 | lr 9.573470e-05\n",
      "| [   1/   3][   300/  4689] |  299/1563 batches | ms/batch 297.9660 | train_loss 8.730540e-01 | lr 9.360205e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   1/   3][   300/  4689] |  299/1563 batches | ms/batch 297.9660 | train_loss 8.730540e-01 | lr 9.360205e-05\n",
      "| [   1/   3][   400/  4689] |  399/1563 batches | ms/batch 300.6760 | train_loss 8.788091e-01 | lr 9.146940e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   1/   3][   400/  4689] |  399/1563 batches | ms/batch 300.6760 | train_loss 8.788091e-01 | lr 9.146940e-05\n",
      "| [   1/   3][   500/  4689] |  499/1563 batches | ms/batch 301.8355 | train_loss 8.647305e-01 | lr 8.933675e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   1/   3][   500/  4689] |  499/1563 batches | ms/batch 301.8355 | train_loss 8.647305e-01 | lr 8.933675e-05\n",
      "| [   1/   3][   600/  4689] |  599/1563 batches | ms/batch 295.7020 | train_loss 8.715992e-01 | lr 8.720409e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   1/   3][   600/  4689] |  599/1563 batches | ms/batch 295.7020 | train_loss 8.715992e-01 | lr 8.720409e-05\n",
      "| [   1/   3][   700/  4689] |  699/1563 batches | ms/batch 303.6058 | train_loss 8.644056e-01 | lr 8.507144e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   1/   3][   700/  4689] |  699/1563 batches | ms/batch 303.6058 | train_loss 8.644056e-01 | lr 8.507144e-05\n",
      "| [   1/   3][   800/  4689] |  799/1563 batches | ms/batch 294.8636 | train_loss 8.593350e-01 | lr 8.293879e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   1/   3][   800/  4689] |  799/1563 batches | ms/batch 294.8636 | train_loss 8.593350e-01 | lr 8.293879e-05\n",
      "| [   1/   3][   900/  4689] |  899/1563 batches | ms/batch 296.2011 | train_loss 8.581630e-01 | lr 8.080614e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   1/   3][   900/  4689] |  899/1563 batches | ms/batch 296.2011 | train_loss 8.581630e-01 | lr 8.080614e-05\n",
      "| [   1/   3][  1000/  4689] |  999/1563 batches | ms/batch 306.0488 | train_loss 8.554698e-01 | lr 7.867349e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   1/   3][  1000/  4689] |  999/1563 batches | ms/batch 306.0488 | train_loss 8.554698e-01 | lr 7.867349e-05\n",
      "| **** saving model (avg_prec=0) to /tmp/tmpht9qjk0k at global_step 1000 ****\n",
      "INFO:pecos.xmc.xtransformer.matcher:| **** saving model (avg_prec=0) to /tmp/tmpht9qjk0k at global_step 1000 ****\n",
      "-----------------------------------------------------------------------------------------\n",
      "INFO:pecos.xmc.xtransformer.matcher:-----------------------------------------------------------------------------------------\n",
      "| [   1/   3][  1100/  4689] | 1099/1563 batches | ms/batch 300.9015 | train_loss 8.495659e-01 | lr 7.654084e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   1/   3][  1100/  4689] | 1099/1563 batches | ms/batch 300.9015 | train_loss 8.495659e-01 | lr 7.654084e-05\n",
      "| [   1/   3][  1200/  4689] | 1199/1563 batches | ms/batch 298.6058 | train_loss 8.540362e-01 | lr 7.440819e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   1/   3][  1200/  4689] | 1199/1563 batches | ms/batch 298.6058 | train_loss 8.540362e-01 | lr 7.440819e-05\n",
      "| [   1/   3][  1300/  4689] | 1299/1563 batches | ms/batch 297.1103 | train_loss 8.463508e-01 | lr 7.227554e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   1/   3][  1300/  4689] | 1299/1563 batches | ms/batch 297.1103 | train_loss 8.463508e-01 | lr 7.227554e-05\n",
      "| [   1/   3][  1400/  4689] | 1399/1563 batches | ms/batch 304.6993 | train_loss 8.483859e-01 | lr 7.014289e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   1/   3][  1400/  4689] | 1399/1563 batches | ms/batch 304.6993 | train_loss 8.483859e-01 | lr 7.014289e-05\n",
      "| [   1/   3][  1500/  4689] | 1499/1563 batches | ms/batch 301.2853 | train_loss 8.438830e-01 | lr 6.801024e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   1/   3][  1500/  4689] | 1499/1563 batches | ms/batch 301.2853 | train_loss 8.438830e-01 | lr 6.801024e-05\n",
      "| [   2/   3][  1600/  4689] |   36/1563 batches | ms/batch 296.6417 | train_loss 8.418559e-01 | lr 6.587759e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  1600/  4689] |   36/1563 batches | ms/batch 296.6417 | train_loss 8.418559e-01 | lr 6.587759e-05\n",
      "| [   2/   3][  1700/  4689] |  136/1563 batches | ms/batch 329.0346 | train_loss 8.383545e-01 | lr 6.374493e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  1700/  4689] |  136/1563 batches | ms/batch 329.0346 | train_loss 8.383545e-01 | lr 6.374493e-05\n",
      "| [   2/   3][  1800/  4689] |  236/1563 batches | ms/batch 319.3022 | train_loss 8.361538e-01 | lr 6.161228e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  1800/  4689] |  236/1563 batches | ms/batch 319.3022 | train_loss 8.361538e-01 | lr 6.161228e-05\n",
      "| [   2/   3][  1900/  4689] |  336/1563 batches | ms/batch 301.3143 | train_loss 8.351288e-01 | lr 5.947963e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  1900/  4689] |  336/1563 batches | ms/batch 301.3143 | train_loss 8.351288e-01 | lr 5.947963e-05\n",
      "| [   2/   3][  2000/  4689] |  436/1563 batches | ms/batch 297.0204 | train_loss 8.368162e-01 | lr 5.734698e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  2000/  4689] |  436/1563 batches | ms/batch 297.0204 | train_loss 8.368162e-01 | lr 5.734698e-05\n",
      "| **** saving model (avg_prec=0) to /tmp/tmpht9qjk0k at global_step 2000 ****\n",
      "INFO:pecos.xmc.xtransformer.matcher:| **** saving model (avg_prec=0) to /tmp/tmpht9qjk0k at global_step 2000 ****\n",
      "-----------------------------------------------------------------------------------------\n",
      "INFO:pecos.xmc.xtransformer.matcher:-----------------------------------------------------------------------------------------\n",
      "| [   2/   3][  2100/  4689] |  536/1563 batches | ms/batch 299.8061 | train_loss 8.361024e-01 | lr 5.521433e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  2100/  4689] |  536/1563 batches | ms/batch 299.8061 | train_loss 8.361024e-01 | lr 5.521433e-05\n",
      "| [   2/   3][  2200/  4689] |  636/1563 batches | ms/batch 302.9627 | train_loss 8.341211e-01 | lr 5.308168e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  2200/  4689] |  636/1563 batches | ms/batch 302.9627 | train_loss 8.341211e-01 | lr 5.308168e-05\n",
      "| [   2/   3][  2300/  4689] |  736/1563 batches | ms/batch 299.5118 | train_loss 8.362810e-01 | lr 5.094903e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  2300/  4689] |  736/1563 batches | ms/batch 299.5118 | train_loss 8.362810e-01 | lr 5.094903e-05\n",
      "| [   2/   3][  2400/  4689] |  836/1563 batches | ms/batch 302.1143 | train_loss 8.306190e-01 | lr 4.881638e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  2400/  4689] |  836/1563 batches | ms/batch 302.1143 | train_loss 8.306190e-01 | lr 4.881638e-05\n",
      "| [   2/   3][  2500/  4689] |  936/1563 batches | ms/batch 300.6434 | train_loss 8.295602e-01 | lr 4.668373e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  2500/  4689] |  936/1563 batches | ms/batch 300.6434 | train_loss 8.295602e-01 | lr 4.668373e-05\n",
      "| [   2/   3][  2600/  4689] | 1036/1563 batches | ms/batch 308.0713 | train_loss 8.350980e-01 | lr 4.455108e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  2600/  4689] | 1036/1563 batches | ms/batch 308.0713 | train_loss 8.350980e-01 | lr 4.455108e-05\n",
      "| [   2/   3][  2700/  4689] | 1136/1563 batches | ms/batch 307.1212 | train_loss 8.314249e-01 | lr 4.241843e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  2700/  4689] | 1136/1563 batches | ms/batch 307.1212 | train_loss 8.314249e-01 | lr 4.241843e-05\n",
      "| [   2/   3][  2800/  4689] | 1236/1563 batches | ms/batch 300.6841 | train_loss 8.267502e-01 | lr 4.028578e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  2800/  4689] | 1236/1563 batches | ms/batch 300.6841 | train_loss 8.267502e-01 | lr 4.028578e-05\n",
      "| [   2/   3][  2900/  4689] | 1336/1563 batches | ms/batch 298.5919 | train_loss 8.286390e-01 | lr 3.815312e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  2900/  4689] | 1336/1563 batches | ms/batch 298.5919 | train_loss 8.286390e-01 | lr 3.815312e-05\n",
      "| [   2/   3][  3000/  4689] | 1436/1563 batches | ms/batch 299.3256 | train_loss 8.259531e-01 | lr 3.602047e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  3000/  4689] | 1436/1563 batches | ms/batch 299.3256 | train_loss 8.259531e-01 | lr 3.602047e-05\n",
      "| **** saving model (avg_prec=0) to /tmp/tmpht9qjk0k at global_step 3000 ****\n",
      "INFO:pecos.xmc.xtransformer.matcher:| **** saving model (avg_prec=0) to /tmp/tmpht9qjk0k at global_step 3000 ****\n",
      "-----------------------------------------------------------------------------------------\n",
      "INFO:pecos.xmc.xtransformer.matcher:-----------------------------------------------------------------------------------------\n",
      "| [   2/   3][  3100/  4689] | 1536/1563 batches | ms/batch 302.3122 | train_loss 8.284464e-01 | lr 3.388782e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   2/   3][  3100/  4689] | 1536/1563 batches | ms/batch 302.3122 | train_loss 8.284464e-01 | lr 3.388782e-05\n",
      "| [   3/   3][  3200/  4689] |   73/1563 batches | ms/batch 304.5938 | train_loss 8.253299e-01 | lr 3.175517e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   3/   3][  3200/  4689] |   73/1563 batches | ms/batch 304.5938 | train_loss 8.253299e-01 | lr 3.175517e-05\n",
      "| [   3/   3][  3300/  4689] |  173/1563 batches | ms/batch 300.7320 | train_loss 8.259755e-01 | lr 2.962252e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   3/   3][  3300/  4689] |  173/1563 batches | ms/batch 300.7320 | train_loss 8.259755e-01 | lr 2.962252e-05\n",
      "| [   3/   3][  3400/  4689] |  273/1563 batches | ms/batch 298.6500 | train_loss 8.239564e-01 | lr 2.748987e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   3/   3][  3400/  4689] |  273/1563 batches | ms/batch 298.6500 | train_loss 8.239564e-01 | lr 2.748987e-05\n",
      "| [   3/   3][  3500/  4689] |  373/1563 batches | ms/batch 300.4057 | train_loss 8.197095e-01 | lr 2.535722e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   3/   3][  3500/  4689] |  373/1563 batches | ms/batch 300.4057 | train_loss 8.197095e-01 | lr 2.535722e-05\n",
      "| [   3/   3][  3600/  4689] |  473/1563 batches | ms/batch 299.5029 | train_loss 8.225163e-01 | lr 2.322457e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   3/   3][  3600/  4689] |  473/1563 batches | ms/batch 299.5029 | train_loss 8.225163e-01 | lr 2.322457e-05\n",
      "| [   3/   3][  3700/  4689] |  573/1563 batches | ms/batch 302.6218 | train_loss 8.200211e-01 | lr 2.109192e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   3/   3][  3700/  4689] |  573/1563 batches | ms/batch 302.6218 | train_loss 8.200211e-01 | lr 2.109192e-05\n",
      "| [   3/   3][  3800/  4689] |  673/1563 batches | ms/batch 313.4861 | train_loss 8.222464e-01 | lr 1.895927e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   3/   3][  3800/  4689] |  673/1563 batches | ms/batch 313.4861 | train_loss 8.222464e-01 | lr 1.895927e-05\n",
      "| [   3/   3][  3900/  4689] |  773/1563 batches | ms/batch 304.7356 | train_loss 8.204356e-01 | lr 1.682662e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   3/   3][  3900/  4689] |  773/1563 batches | ms/batch 304.7356 | train_loss 8.204356e-01 | lr 1.682662e-05\n",
      "| [   3/   3][  4000/  4689] |  873/1563 batches | ms/batch 307.0356 | train_loss 8.182005e-01 | lr 1.469396e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   3/   3][  4000/  4689] |  873/1563 batches | ms/batch 307.0356 | train_loss 8.182005e-01 | lr 1.469396e-05\n",
      "| **** saving model (avg_prec=0) to /tmp/tmpht9qjk0k at global_step 4000 ****\n",
      "INFO:pecos.xmc.xtransformer.matcher:| **** saving model (avg_prec=0) to /tmp/tmpht9qjk0k at global_step 4000 ****\n",
      "-----------------------------------------------------------------------------------------\n",
      "INFO:pecos.xmc.xtransformer.matcher:-----------------------------------------------------------------------------------------\n",
      "| [   3/   3][  4100/  4689] |  973/1563 batches | ms/batch 320.3147 | train_loss 8.184975e-01 | lr 1.256131e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   3/   3][  4100/  4689] |  973/1563 batches | ms/batch 320.3147 | train_loss 8.184975e-01 | lr 1.256131e-05\n",
      "| [   3/   3][  4200/  4689] | 1073/1563 batches | ms/batch 327.8610 | train_loss 8.157151e-01 | lr 1.042866e-05\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   3/   3][  4200/  4689] | 1073/1563 batches | ms/batch 327.8610 | train_loss 8.157151e-01 | lr 1.042866e-05\n",
      "| [   3/   3][  4300/  4689] | 1173/1563 batches | ms/batch 317.3419 | train_loss 8.199388e-01 | lr 8.296012e-06\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   3/   3][  4300/  4689] | 1173/1563 batches | ms/batch 317.3419 | train_loss 8.199388e-01 | lr 8.296012e-06\n",
      "| [   3/   3][  4400/  4689] | 1273/1563 batches | ms/batch 317.0017 | train_loss 8.158111e-01 | lr 6.163361e-06\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   3/   3][  4400/  4689] | 1273/1563 batches | ms/batch 317.0017 | train_loss 8.158111e-01 | lr 6.163361e-06\n",
      "| [   3/   3][  4500/  4689] | 1373/1563 batches | ms/batch 316.8681 | train_loss 8.133276e-01 | lr 4.030710e-06\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   3/   3][  4500/  4689] | 1373/1563 batches | ms/batch 316.8681 | train_loss 8.133276e-01 | lr 4.030710e-06\n",
      "| [   3/   3][  4600/  4689] | 1473/1563 batches | ms/batch 317.0055 | train_loss 8.151729e-01 | lr 1.898059e-06\n",
      "INFO:pecos.xmc.xtransformer.matcher:| [   3/   3][  4600/  4689] | 1473/1563 batches | ms/batch 317.0055 | train_loss 8.151729e-01 | lr 1.898059e-06\n",
      "Reload the best checkpoint from /tmp/tmpht9qjk0k\n",
      "INFO:pecos.xmc.xtransformer.matcher:Reload the best checkpoint from /tmp/tmpht9qjk0k\n",
      "Predict with csr_codes_next((50000, 8192)) with avr_nnz=320.0\n",
      "INFO:pecos.xmc.xtransformer.matcher:Predict with csr_codes_next((50000, 8192)) with avr_nnz=320.0\n"
     ]
    }
   ],
   "source": [
    "!annif train lcsh-xtransformer-distilbert-en datasets/xtf_train_summaries_uri.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the model for 3 epochs, it can be evaluated using the regular test_summaries dataset. `2>/dev/null` is added to suppress CUDA user warnings, since it will produce one warning for every record in the test set, this would cause 1000+ warning messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision (doc avg):          \t0.1707\n",
      "Recall (doc avg):             \t0.5205\n",
      "F1 score (doc avg):           \t0.2441\n",
      "Precision (subj avg):         \t0.0005\n",
      "Recall (subj avg):            \t0.0009\n",
      "F1 score (subj avg):          \t0.0005\n",
      "Precision (weighted subj avg):\t0.2746\n",
      "Recall (weighted subj avg):   \t0.4970\n",
      "F1 score (weighted subj avg): \t0.3330\n",
      "Precision (microavg):         \t0.1707\n",
      "Recall (microavg):            \t0.4970\n",
      "F1 score (microavg):          \t0.2541\n",
      "F1@5:                         \t0.3251\n",
      "NDCG:                         \t0.4870\n",
      "NDCG@5:                       \t0.4618\n",
      "NDCG@10:                      \t0.4870\n",
      "Precision@1:                  \t0.5386\n",
      "Precision@3:                  \t0.3740\n",
      "Precision@5:                  \t0.2848\n",
      "True positives:               \t7490\n",
      "False positives:              \t36400\n",
      "False negatives:              \t7579\n",
      "Documents evaluated:          \t4389\n"
     ]
    }
   ],
   "source": [
    "!annif eval lcsh-xtransformer-distilbert-en datasets/test_summaries_uri.tsv 2>/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This transformer model which makes use of distilbert gets a NDCG score of 0.4870. This is worse then Omikuji Parabel. This could be due to the fact that is is trained on a lot less data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example test\n",
    "Below I am testing all models on a simple query, to give an example of the usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### TF-IDF (Titles)\n",
    "TF-IDF which is trained on titles gives promising subjects, but as seen it gives some weird subjects on this query, such as _Dreams and the Arts_. A explanation for this might be because the first part of the query: \"What can you tell\" is associated with that subject. And since it is trained on small texts (titles) it associates those otherwise neutral words with that subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<http://id.loc.gov/authorities/subjects/sh2003001414>\tDreams and the arts\t0.3677\n",
      "<http://id.loc.gov/authorities/subjects/sh85079324>\tMachine learning\t0.3652\n",
      "<http://id.loc.gov/authorities/subjects/sh2002007921>\tMathematical models\t0.2907\n",
      "<http://id.loc.gov/authorities/subjects/sh85082123>\tMathematical literature\t0.2900\n",
      "<http://id.loc.gov/authorities/subjects/sh91000149>\tComputer algorithms\t0.2834\n",
      "<http://id.loc.gov/authorities/subjects/sh90004643>\tHispanic Americans in motion pictures\t0.2772\n",
      "<http://id.loc.gov/authorities/subjects/sh85075520>\tLearning\t0.2758\n",
      "<http://id.loc.gov/authorities/subjects/sh2006000311>\tAfrican American fraternal organizations\t0.2738\n",
      "<http://id.loc.gov/authorities/subjects/sh85082139>\tMathematics\t0.2699\n",
      "<http://id.loc.gov/authorities/subjects/sh85014235>\tBiomathematics\t0.2634\n"
     ]
    }
   ],
   "source": [
    "!echo \"What can you tell me about Machine learning algorithms, which use mathematical models\" | annif suggest lcsh-tfidf-en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### TF-IDF (Summaries)\n",
    "This TF-IDF model is trained on the summaries. And as seen it produces different results. Here there are also some weird subjects, but all in all it performs better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<http://id.loc.gov/authorities/subjects/sh85093206>\tNumber concept\t0.2988\n",
      "<http://id.loc.gov/authorities/subjects/sh85077449>\tLISP (Computer program language)\t0.2898\n",
      "<http://id.loc.gov/authorities/subjects/sh96008834>\tPython (Computer program language)\t0.2865\n",
      "<http://id.loc.gov/authorities/subjects/sh92006189>\tWomen periodical editors\t0.2853\n",
      "<http://id.loc.gov/authorities/subjects/sh98004629>\tLibraries and distance education\t0.2628\n",
      "<http://id.loc.gov/authorities/subjects/sh99003437>\tOpen source software\t0.2609\n",
      "<http://id.loc.gov/authorities/subjects/sh87007505>\tC++ (Computer program language)\t0.2384\n",
      "<http://id.loc.gov/authorities/subjects/sh2014000535>\tMakerspaces\t0.2343\n",
      "<http://id.loc.gov/authorities/subjects/sh85042371>\tElectronic spreadsheets\t0.2252\n",
      "<http://id.loc.gov/authorities/subjects/sh91000149>\tComputer algorithms\t0.2240\n"
     ]
    }
   ],
   "source": [
    "!echo \"What can you tell me about Machine learning algorithms, which use mathematical models\" | annif suggest lcsh-tfidf2-en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Omikuji Parabel\n",
    "The next model is the Omikuji Parabel model. This uses a whole different technique as mentioned before. And as can be seen from the subjects, they are all relevant for the search query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-14T15:25:01.588Z \u001b[36mINFO \u001b[0m [omikuji::model] Loading model from data/projects/lcsh-omikuji-parabel-en/omikuji-model...\n",
      "2024-01-14T15:25:01.588Z \u001b[36mINFO \u001b[0m [omikuji::model] Loading model settings from data/projects/lcsh-omikuji-parabel-en/omikuji-model/settings.json...\n",
      "2024-01-14T15:25:01.594Z \u001b[36mINFO \u001b[0m [omikuji::model] Loaded model settings Settings { n_features: 336785, classifier_loss_type: Hinge }...\n",
      "2024-01-14T15:25:01.598Z \u001b[36mINFO \u001b[0m [omikuji::model] Loading tree from data/projects/lcsh-omikuji-parabel-en/omikuji-model/tree0.cbor...\n",
      "2024-01-14T15:25:04.023Z \u001b[36mINFO \u001b[0m [omikuji::model] Loading tree from data/projects/lcsh-omikuji-parabel-en/omikuji-model/tree1.cbor...\n",
      "2024-01-14T15:25:06.424Z \u001b[36mINFO \u001b[0m [omikuji::model] Loading tree from data/projects/lcsh-omikuji-parabel-en/omikuji-model/tree2.cbor...\n",
      "2024-01-14T15:25:08.898Z \u001b[36mINFO \u001b[0m [omikuji::model] Loaded model with 3 trees; it took 7.31s\n",
      "<http://id.loc.gov/authorities/subjects/sh85008180>\tArtificial intelligence\t0.1802\n",
      "<http://id.loc.gov/authorities/subjects/sh85079324>\tMachine learning\t0.1706\n",
      "<http://id.loc.gov/authorities/subjects/sh85082139>\tMathematics\t0.1428\n",
      "<http://id.loc.gov/authorities/subjects/sh89003285>\tComputer science\t0.0780\n",
      "<http://id.loc.gov/authorities/subjects/sh85043176>\tEngineering\t0.0638\n",
      "<http://id.loc.gov/authorities/subjects/sh85029552>\tComputers\t0.0589\n",
      "<http://id.loc.gov/authorities/subjects/sh99003437>\tOpen source software\t0.0571\n",
      "<http://id.loc.gov/authorities/subjects/sh99005294>\tComputer networks\t0.0494\n",
      "<http://id.loc.gov/authorities/subjects/sh2012003227>\tBig data\t0.0454\n",
      "<http://id.loc.gov/authorities/subjects/sh94004659>\tComputational intelligence\t0.0417\n"
     ]
    }
   ],
   "source": [
    "!echo \"What can you tell me about Machine learning algorithms, which use mathematical models\" | annif suggest lcsh-omikuji-parabel-en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### XTransformer\n",
    "Lastly we have the XTransformer, this model has a similar performance to the Omikuji model. It is evident that these models have a better understanding of the query as opposed to simpler models like TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:pecos.utils.torch_util:CUDA is available but disabled, will only use CPU.\n",
      "<http://id.loc.gov/authorities/subjects/sh85079324>\tMachine learning\t0.1592\n",
      "<http://id.loc.gov/authorities/subjects/sh89003285>\tComputer science\t0.1273\n",
      "<http://id.loc.gov/authorities/subjects/sh85008180>\tArtificial intelligence\t0.1177\n",
      "<http://id.loc.gov/authorities/subjects/sh85079341>\tMachine theory\t0.1171\n",
      "<http://id.loc.gov/authorities/subjects/sh85010089>\tAutomatic control\t0.1136\n",
      "<http://id.loc.gov/authorities/subjects/sh85114628>\tRobotics\t0.1086\n",
      "<http://id.loc.gov/authorities/subjects/sh87007398>\tSoftware engineering\t0.1072\n",
      "<http://id.loc.gov/authorities/subjects/sh85043176>\tEngineering\t0.1012\n",
      "<http://id.loc.gov/authorities/subjects/sh85107313>\tProgramming languages (Electronic computers)\t0.1006\n",
      "<http://id.loc.gov/authorities/subjects/sh00005852>\tQuality control\t0.0962\n"
     ]
    }
   ],
   "source": [
    "!echo \"What can you tell me about Machine learning algorithms, which use mathematical models\" | annif suggest lcsh-xtransformer-distilbert-en"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
